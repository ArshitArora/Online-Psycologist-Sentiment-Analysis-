{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# text preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# plots and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arshitarora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of labels: joy, anger, fear, sadness, neutral\n",
    "num_classes = 5\n",
    "\n",
    "# Number of dimensions for word embedding\n",
    "embed_num_dims = 300\n",
    "\n",
    "# Max input length (max number of words) \n",
    "max_seq_len = 500\n",
    "\n",
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/xt3hvz6100vf2k3q4zcq57km0000gn/T/ipykernel_21493/1066771544.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data_train.append(data_test, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/data_train.csv', encoding='utf-8')\n",
    "data_test = pd.read_csv('data/data_test.csv', encoding='utf-8')\n",
    "\n",
    "X_train = data_train.Text\n",
    "X_test = data_test.Text\n",
    "\n",
    "y_train = data_train.Emotion\n",
    "y_test = data_test.Emotion\n",
    "\n",
    "data = data_train.append(data_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        2326\n",
      "sadness    2317\n",
      "anger      2259\n",
      "neutral    2254\n",
      "fear       2171\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Yet the dog had grown old and less capable , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>She disliked the intimacy he showed towards so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When my family heard that my Mother's cousin w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral   There are tons of other paintings that I thin...\n",
       "1  sadness  Yet the dog had grown old and less capable , a...\n",
       "2     fear  When I get into the tube or the train without ...\n",
       "3     fear  This last may be a source of considerable disq...\n",
       "4    anger  She disliked the intimacy he showed towards so...\n",
       "5  sadness  When my family heard that my Mother's cousin w..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.Emotion.value_counts())\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \n",
    "    # remove hashtags and @usernames\n",
    "    data = re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data = re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "    \n",
    "    # tekenization using nltk\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(clean_text(text)) for text in data.Text]\n",
    "\n",
    "texts_train = [' '.join(clean_text(text)) for text in X_train]\n",
    "texts_test = [' '.join(clean_text(text)) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bit ? I 'm extremely annoyed that he did n't phone me when he promised me that he would ! He 's such a liar .\n"
     ]
    }
   ],
   "source": [
    "print(texts_train[92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 12087\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "# vacab size is number of unique words + reserved 0 index for padding\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   119,    51,   345],\n",
       "       [    0,     0,     0, ...,    37,   277,   154],\n",
       "       [    0,     0,     0, ...,    16,     2,  1210],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   876,     4,   909],\n",
       "       [    0,     0,     0, ...,     1,     6,   117],\n",
       "       [    0,     0,     0, ..., 10258,   173,    13]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
    "\n",
    "X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'joy': 0,\n",
    "    'fear': 1,\n",
    "    'anger': 2,\n",
    "    'sadness': 3,\n",
    "    'neutral': 4\n",
    "}\n",
    "\n",
    "# Integer labels\n",
    "y_train = [encoding[x] for x in data_train.Emotion]\n",
    "y_test = [encoding[x] for x in data_test.Emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    print('Downloading word vectors...')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
    "                              'wiki-news-300d-1M.vec.zip')\n",
    "    print('Unzipping...')\n",
    "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('embeddings')\n",
    "    print('done.')\n",
    "    \n",
    "    os.remove('wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12088, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
    "embedd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in wiki vocab: 11442\n",
      "New words found: 645\n"
     ]
    }
   ],
   "source": [
    "# Inspect unseen words\n",
    "new_words = 0\n",
    "\n",
    "for word in index_of_words:\n",
    "    entry = embedd_matrix[index_of_words[word]]\n",
    "    if all(v == 0 for v in entry):\n",
    "        new_words = new_words + 1\n",
    "\n",
    "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
    "print('New words found: ' + str(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = max_seq_len,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "kernel_size = 3\n",
    "filters = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 300)          3626400   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 498, 256)          230656    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,924,133\n",
      "Trainable params: 297,733\n",
      "Non-trainable params: 3,626,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 01:19:12.895028: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 18s 280ms/step - loss: 1.1994 - accuracy: 0.5314 - val_loss: 0.8732 - val_accuracy: 0.6908\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 18s 293ms/step - loss: 0.6952 - accuracy: 0.7559 - val_loss: 0.7349 - val_accuracy: 0.7401\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 18s 290ms/step - loss: 0.5146 - accuracy: 0.8267 - val_loss: 0.7754 - val_accuracy: 0.7362\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 18s 289ms/step - loss: 0.3825 - accuracy: 0.8837 - val_loss: 0.7024 - val_accuracy: 0.7557\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 19s 301ms/step - loss: 0.2633 - accuracy: 0.9308 - val_loss: 0.7450 - val_accuracy: 0.7451\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 19s 312ms/step - loss: 0.1787 - accuracy: 0.9598 - val_loss: 0.7436 - val_accuracy: 0.7616\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 17s 279ms/step - loss: 0.1119 - accuracy: 0.9815 - val_loss: 0.7647 - val_accuracy: 0.7571\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 17s 283ms/step - loss: 0.0715 - accuracy: 0.9892 - val_loss: 0.8062 - val_accuracy: 0.7613\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 19s 305ms/step - loss: 0.0546 - accuracy: 0.9914 - val_loss: 0.8462 - val_accuracy: 0.7554\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 19s 299ms/step - loss: 0.0485 - accuracy: 0.9919 - val_loss: 0.8643 - val_accuracy: 0.7539\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "hist = model.fit(X_train_pad, y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 3s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_pad)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions = [class_names[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43maccuracy_score\u001b[49m(data_test\u001b[38;5;241m.\u001b[39mEmotion, predictions) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1_score(data_test\u001b[38;5;241m.\u001b[39mEmotion, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(data_test.Emotion, predictions) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Set size\n",
    "    fig.set_size_inches(12.5, 7.5)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try other inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: My boyfriend didn't turn up after promising that he was coming.\n",
      "Predicted: sadness\n"
     ]
    }
   ],
   "source": [
    "print('Message: {}\\nPredicted: {}'.format(X_test[4], predictions[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 368, 54, 18, 7]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[[2.7704720e-05 1.5836453e-02 1.7403776e-02 9.6613353e-01 5.9855753e-04]]\n",
      "Message: ['I broke up. with my gf']\n",
      "predicted: sadness (0.04 seconds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "message = ['I broke up. with my gf']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "print(seq)\n",
    "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
    "\n",
    "start_time = time.time()\n",
    "pred = model.predict(padded)\n",
    "print(pred)\n",
    "\n",
    "print('Message: ' + str(message))\n",
    "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done\n",
    "Save the model for later use 🙃 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save('models/cnn_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "predictor = load_model('models/cnn_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (3.11.0)\n",
      "Requirement already satisfied: matplotlib in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (3.5.3)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: numpy in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (1.23.4)\n",
      "Requirement already satisfied: pyyaml in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: orjson in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (3.8.2)\n",
      "Requirement already satisfied: fsspec in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (2022.11.0)\n",
      "Requirement already satisfied: pydantic in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (1.10.2)\n",
      "Requirement already satisfied: pandas in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (1.5.1)\n",
      "Requirement already satisfied: pydub in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: requests in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: websockets>=10.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (10.4)\n",
      "Requirement already satisfied: pillow in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: fastapi in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.88.0)\n",
      "Requirement already satisfied: uvicorn in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: pycryptodome in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (3.16.0)\n",
      "Requirement already satisfied: markdown-it-py[linkify,plugins] in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: python-multipart in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: ffmpy in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: httpx in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (0.23.1)\n",
      "Requirement already satisfied: paramiko in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (2.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from gradio) (3.8.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from aiohttp->gradio) (6.0.2)\n",
      "Requirement already satisfied: starlette==0.22.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from starlette==0.22.0->fastapi->gradio) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from pydantic->gradio) (4.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from httpx->gradio) (0.15.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from httpx->gradio) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from httpx->gradio) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from jinja2->gradio) (2.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py~=1.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from pandas->gradio) (2022.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from paramiko->gradio) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from paramiko->gradio) (38.0.1)\n",
      "Requirement already satisfied: six in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from paramiko->gradio) (1.16.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from paramiko->gradio) (1.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from requests->gradio) (1.26.12)\n",
      "Requirement already satisfied: click>=7.0 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from uvicorn->gradio) (8.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
      "Requirement already satisfied: uc-micro-py in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
      "Requirement already satisfied: pycparser in /Users/arshitarora/opt/miniconda3/envs/condajupyter/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(text):\n",
    "    class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']\n",
    "    start_time = time.time()\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_seq_len)\n",
    "    pred = model.predict(padded)\n",
    "    return class_names[np.argmax(pred)]\n",
    "\n",
    "\n",
    "demo = gr.Interface(predict, [gr.Textbox(placeholder='Enter a text')], \"text\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
